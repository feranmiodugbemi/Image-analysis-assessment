{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Analysis and Object Detection Program\n",
        "\n",
        "## Overview\n",
        "\n",
        "This program uses advanced machine learning models to analyze images, detect objects, and generate detailed descriptions. It combines object detection models (DETR and YOLOS) with an image captioning model (BLIP) to provide a comprehensive analysis of input images.\n",
        "\n",
        "## Features\n",
        "\n",
        "1. Object detection using either DETR or YOLOS models\n",
        "2. Image captioning using the BLIP model\n",
        "3. Color extraction from detected objects\n",
        "4. Activity detection for objects\n",
        "5. Support for both local image files and image URLs\n",
        "\n",
        "## Requirements\n",
        "\n",
        "To run this program, you need:\n",
        "\n",
        "- Python 3.7+\n",
        "- PyTorch\n",
        "- Transformers library\n",
        "- PIL (Python Imaging Library)\n",
        "- OpenCV (cv2)\n",
        "- NumPy\n",
        "- scikit-learn\n",
        "- webcolors\n",
        "- requests\n",
        "- Recommended to use Google colab, you need a RAM of up to 12GB to run both models\n",
        "\n",
        "You can install the required libraries using pip, which is included in this ipynb file:\n",
        "\n",
        "```\n",
        "pip install torch transformers Pillow opencv-python numpy scikit-learn webcolors requests\n",
        "```\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Image Input**: The program accepts either a local file path or a URL to an image.\n",
        "\n",
        "2. **Image Processing**: The input image is opened and converted to a NumPy array for processing.\n",
        "\n",
        "3. **Object Detection**: Depending on which version you run, either the DETR or YOLOS model is used to detect objects in the image. Both models return bounding boxes and labels for detected objects.\n",
        "\n",
        "4. **Image Captioning**: The BLIP model is used to generate captions for the entire image and for individual detected objects.\n",
        "\n",
        "5. **Color and Activity Extraction**: The program analyzes the captions to extract color information and potential activities associated with the objects.\n",
        "\n",
        "6. **Vision Record Generation**: All the gathered information is compiled into a \"vision record\" - a dictionary containing details about detected objects, their colors, activities, bounding boxes, and an overall image summary.\n",
        "\n",
        "## How to Run\n",
        "\n",
        "1. Ensure all required libraries are installed.\n",
        "\n",
        "2. Run the each of the cells in the notebook. You will be prompted to enter the path to your image or a URL:\n",
        "\n",
        "3. When prompted, enter either a local file path or an image URL:\n",
        "\n",
        "   ```\n",
        "   Input path to your image (from internet or path): /path/to/your/image.jpg\n",
        "   ```\n",
        "   or\n",
        "   ```\n",
        "   Input path to your image (from internet or path): https://example.com/image.jpg\n",
        "   ```\n",
        "\n",
        "4. The program will process the image and output a vision record containing all the analyzed information.\n",
        "\n",
        "## Output\n",
        "\n",
        "The program outputs a vision record dictionary with the following information:\n",
        "\n",
        "- Time: Timestamp of the analysis\n",
        "- Objects: List of detected objects\n",
        "- Objects Activities: List of activities associated with each object (if any)\n",
        "- Object Colors: List of colors associated with each object\n",
        "- Object Bounding Boxes: Coordinates of bounding boxes for each detected object\n",
        "- Frame Size: Dimensions of the input image\n",
        "- Frame Summary: Overall description of the image\n",
        "\n",
        "## Note on Model Versions\n",
        "\n",
        "The code includes two versions of the `generate_vision_record` function:\n",
        "\n",
        "1. Using the DETR (DEtection TRansformer) model\n",
        "2. Using the YOLOS (You Only Look at One Sequence) model\n",
        "\n",
        "By default, the main function uses the YOLOS version. To switch to the DETR version, you would need to modify the `main()` function to call the DETR version of `generate_vision_record`.\n",
        "\n",
        "## Limitations and Considerations\n",
        "\n",
        "- The accuracy of object detection, color extraction, and activity recognition depends on the training and capabilities of the underlying models.\n",
        "- Processing large images or running the program on a CPU can be time-consuming. A GPU is recommended for faster processing, though this notebook wasn't optimized for GPU usage.\n",
        "- The program requires an internet connection to download the model weights if they're not already cached.\n"
      ],
      "metadata": {
        "id": "NFRO_ulQklEl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daJxYg0xU_3w"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless tensorflow numpy Pillow accelerate flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7p1dWuYd9ss"
      },
      "outputs": [],
      "source": [
        "!pip install transformers requests torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC5xoj3dp9LW"
      },
      "outputs": [],
      "source": [
        "!pip install webcolors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "L6PHWvF87rjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v_bBzxTffa5I"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import DetrImageProcessor, DetrForObjectDetection, BlipProcessor, BlipForConditionalGeneration, YolosImageProcessor, YolosForObjectDetection\n",
        "from PIL import Image\n",
        "import warnings\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "import datetime\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "import webcolors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-8h8g_y9favZ"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "def is_url(s):\n",
        "    \"\"\"\n",
        "    Check if a given string is a valid URL for an image.\n",
        "\n",
        "    Args:\n",
        "    s (str): The string to check.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the string is a valid image URL, False otherwise.\n",
        "    \"\"\"\n",
        "    # Regex pattern to check if the string is a URL\n",
        "    url_pattern = r'^https?:\\/\\/(?:www\\.)?.*\\.(jpg|jpeg|png|gif|bmp|tiff|webp)$'\n",
        "    return re.match(url_pattern, s) is not None\n",
        "\n",
        "def open_image(image_source: str):\n",
        "    \"\"\"\n",
        "    Open an image from a URL or file path.\n",
        "\n",
        "    Args:\n",
        "    image_source (str): URL or file path of the image.\n",
        "\n",
        "    Returns:\n",
        "    numpy.array: The image as a NumPy array, or None if there's an error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if is_url(image_source):\n",
        "            # Open the image from a URL\n",
        "            response = requests.get(image_source, stream=True)\n",
        "            response.raise_for_status()  # Check if the request was successful\n",
        "            image_bytes = io.BytesIO(response.content)\n",
        "            image = Image.open(image_bytes).convert(\"RGB\")\n",
        "        else:\n",
        "            # Open the image from a file path\n",
        "            image = Image.open(image_source).convert(\"RGB\")\n",
        "\n",
        "        return np.array(image)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching image from URL: {e}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error opening image file: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def crop_image(image: np.array, coordinates):\n",
        "    \"\"\"\n",
        "    Crop an image using given coordinates.\n",
        "\n",
        "    Args:\n",
        "    image (numpy.array): The input image as a NumPy array.\n",
        "    coordinates (tuple): The coordinates for cropping (left, top, right, bottom).\n",
        "\n",
        "    Returns:\n",
        "    numpy.array: The cropped image as a NumPy array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert NumPy array back to PIL Image\n",
        "    image_pil = Image.fromarray(image)\n",
        "\n",
        "    # Unpack coordinates\n",
        "    left, top, right, bottom = coordinates\n",
        "\n",
        "    # Crop the image using the coordinates\n",
        "    cropped_image_pil = image_pil.crop((left, top, right, bottom))\n",
        "\n",
        "    # Convert cropped image back to NumPy array\n",
        "    return np.array(cropped_image_pil)\n",
        "\n",
        "\n",
        "def extract_colors(sentence):\n",
        "    \"\"\"\n",
        "    Extract color names from a given sentence.\n",
        "\n",
        "    Args:\n",
        "    sentence (str): The input sentence.\n",
        "\n",
        "    Returns:\n",
        "    str: A string of extracted colors or 'colorless' if no colors are found.\n",
        "    \"\"\"\n",
        "\n",
        "    # List of common colors\n",
        "    color_list = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown', 'black', 'white', 'gray', 'grey', 'silver', 'gold', 'colorless']\n",
        "\n",
        "    # Convert sentence to lowercase for case-insensitive matching\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Find all color words in the sentence\n",
        "    found_colors = [color for color in color_list if color in sentence]\n",
        "\n",
        "    # If no colors found, return 'colorless'\n",
        "    if not found_colors:\n",
        "        return 'colorless'\n",
        "\n",
        "    # If only one color found, return it\n",
        "    if len(found_colors) == 1:\n",
        "        return found_colors[0]\n",
        "\n",
        "    # If multiple colors found, format them\n",
        "    if len(found_colors) == 2:\n",
        "        return f\"{found_colors[0]} and {found_colors[1]}\"\n",
        "    else:\n",
        "        return \", \".join(found_colors[:-1]) + f\", and {found_colors[-1]}\"\n",
        "\n",
        "def extract_activity(sentence):\n",
        "    \"\"\"\n",
        "    Extract activity words (ending with 'ing') from a given sentence.\n",
        "\n",
        "    Args:\n",
        "    sentence (str): The input sentence.\n",
        "\n",
        "    Returns:\n",
        "    str: The first activity word found, or None if no activity is found.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert sentence to lowercase for case-insensitive matching\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Find all words ending with 'ing'\n",
        "    ing_words = re.findall(r'\\b\\w+ing\\b', sentence)\n",
        "\n",
        "    # If 'ing' words found, return the first one\n",
        "    if ing_words:\n",
        "        return ing_words[0]\n",
        "\n",
        "    # Special case for 'pointing at'\n",
        "    if 'pointing at' in sentence:\n",
        "        return 'pointing'\n",
        "\n",
        "    # If no activity found, return None\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8jOkf8-hqaX"
      },
      "outputs": [],
      "source": [
        "# Model 2\n",
        "\n",
        "\n",
        "# Load model and processor once\n",
        "model_name = \"Salesforce/blip-image-captioning-large\"\n",
        "\n",
        "# Disable some warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load model and processor\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def model_two(image_url: np.array, conditional_prompt: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Generate image captions using the BLIP model.\n",
        "\n",
        "    Args:\n",
        "    image_url (numpy.array): The input image as a NumPy array.\n",
        "    conditional_prompt (str, optional): A conditional prompt for captioning.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing unconditional and (if provided) conditional captions.\n",
        "    \"\"\"\n",
        "\n",
        "    raw_image = Image.fromarray(image_url)\n",
        "    inputs_unconditional = processor(raw_image, return_tensors=\"pt\")\n",
        "    out_unconditional = model.generate(**inputs_unconditional)\n",
        "    caption_unconditional = processor.decode(out_unconditional[0], skip_special_tokens=True)\n",
        "\n",
        "    results = {\"unconditional\": caption_unconditional}\n",
        "\n",
        "    if conditional_prompt:\n",
        "        # Conditional image captioning\n",
        "        inputs_conditional = processor(raw_image, conditional_prompt, return_tensors=\"pt\")\n",
        "        out_conditional = model.generate(**inputs_conditional)\n",
        "        caption_conditional = processor.decode(out_conditional[0], skip_special_tokens=True)\n",
        "        results[\"conditional\"] = caption_conditional\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eadorQtZEyI"
      },
      "outputs": [],
      "source": [
        "def generate_vision_record(image: np.array) -> dict:\n",
        "    \"\"\"\n",
        "    Generate a vision record using the DETR object detection model and BLIP captioning.\n",
        "\n",
        "    Args:\n",
        "    image (numpy.array): The input image as a NumPy array.\n",
        "\n",
        "    Returns:\n",
        "    dict: A vision record containing detected objects, activities, colors, and other information.\n",
        "    \"\"\"\n",
        "\n",
        "    result = None\n",
        "    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
        "    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
        "\n",
        "    # Convert NumPy array to PIL Image for processing\n",
        "    image_pil = Image.fromarray(image)\n",
        "\n",
        "    # Prepare the image for detection\n",
        "    inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Convert outputs (bounding boxes and class logits) to COCO API\n",
        "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
        "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
        "\n",
        "    # Initialize the vision record dictionary\n",
        "    vision_record = {\n",
        "        \"Time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n",
        "        \"Objects\": [],\n",
        "        \"Objects Activities\": [],\n",
        "        \"Object Colors\": [],\n",
        "        \"Object Bounding Boxes\": [],\n",
        "        \"Frame Size\": image.shape[:2],  # (height, width)\n",
        "        \"Frame Summary\" : \"\"\n",
        "    }\n",
        "\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        box = [round(i, 2) for i in box.tolist()]\n",
        "        result = \"Detected\"\n",
        "        # Crop image using bounding box coordinates\n",
        "        cropped_image = crop_image(image, box)\n",
        "        response_one = model_two(image, \"This image depicts\")\n",
        "        response_two_one = model_two(cropped_image, \"This object(s) is color\")\n",
        "        response_two_two = model_two(cropped_image, \"This object(s) is\")\n",
        "\n",
        "        summary = response_one.get(\"unconditional\")\n",
        "        color = extract_colors(response_two_one.get(\"unconditional\"))\n",
        "        activity = extract_activity(response_two_two.get(\"unconditional\"))\n",
        "\n",
        "        vision_record[\"Objects\"].append(model.config.id2label[label.item()])\n",
        "        vision_record[\"Objects Activities\"].append(activity)\n",
        "        vision_record[\"Object Colors\"].append(color)\n",
        "        vision_record[\"Object Bounding Boxes\"].append(box)\n",
        "        vision_record[\"Frame Summary\"] = summary\n",
        "    return vision_record\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to process an image and generate a vision record using the DETR model.\n",
        "    \"\"\"\n",
        "    image_source = input(\"Input path to your image (from internet or path): \")\n",
        "    image = open_image(image_source)\n",
        "    vision_record = generate_vision_record(image)\n",
        "    print(vision_record)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DznIZ487XfwU"
      },
      "outputs": [],
      "source": [
        "def generate_vision_record(image: np.array):\n",
        "    \"\"\"\n",
        "    Generate a vision record using the YOLOS object detection model and BLIP captioning.\n",
        "\n",
        "    Args:\n",
        "    image (numpy.array): The input image as a NumPy array.\n",
        "\n",
        "    Returns:\n",
        "    dict: A vision record containing detected objects, activities, colors, and other information.\n",
        "    \"\"\"\n",
        "\n",
        "    result = None\n",
        "    model = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\n",
        "    image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-small\")\n",
        "\n",
        "    # Convert NumPy array to PIL Image for processing\n",
        "    image_pil = Image.fromarray(image)\n",
        "\n",
        "    inputs = image_processor(images=image_pil, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Process the outputs\n",
        "    target_sizes = torch.tensor([image_pil.size[::-1]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
        "\n",
        "    # Initialize the vision record dictionary\n",
        "    vision_record = {\n",
        "        \"Time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n",
        "        \"Objects\": [],\n",
        "        \"Objects Activities\": [],\n",
        "        \"Object Colors\": [],\n",
        "        \"Object Bounding Boxes\": [],\n",
        "        \"Frame Size\": image.shape[:2],  # (height, width)\n",
        "        \"Frame Summary\" : \"\"\n",
        "    }\n",
        "\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        box = [round(i, 2) for i in box.tolist()]\n",
        "        result = \"Detected\"\n",
        "        # Crop image using bounding box coordinates\n",
        "        cropped_image = crop_image(image, box)\n",
        "        response_one = model_two(image, \"This image depicts\")\n",
        "        response_two_one = model_two(cropped_image, \"This object(s) is color\")\n",
        "        response_two_two = model_two(cropped_image, \"This object(s) is\")\n",
        "\n",
        "        summary = response_one.get(\"unconditional\")\n",
        "        color = extract_colors(response_two_one.get(\"unconditional\"))\n",
        "        activity = extract_activity(response_two_two.get(\"unconditional\"))\n",
        "\n",
        "        vision_record[\"Objects\"].append(model.config.id2label[label.item()])\n",
        "        vision_record[\"Objects Activities\"].append(activity)\n",
        "        vision_record[\"Object Colors\"].append(color)\n",
        "        vision_record[\"Object Bounding Boxes\"].append(box)\n",
        "        vision_record[\"Frame Summary\"] = summary\n",
        "    return vision_record\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to process an image and generate a vision record using the YOLOS model.\n",
        "    \"\"\"\n",
        "    image_source = input(\"Input path to your image (from internet or path): \")\n",
        "    image = open_image(image_source)\n",
        "    vision_record = generate_vision_record(image)\n",
        "    print(vision_record)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}